#!/usr/bin/env python3
"""
æµ‹è¯•å¢å¼ºçš„æ ¸å¿ƒä¼˜åŒ–ç®—æ³•åŠŸèƒ½
éªŒè¯Week 3 Day 3-5çš„æ ¸å¿ƒä¼˜åŒ–ç®—æ³•å®ç°
"""
import asyncio
import sys
import os
sys.path.append('.')

from app.core.ai_client import AIClient
from app.core.prompt_optimizer import AIPromptOptimizer, OptimizationContext
from app.core.quality_evaluator import QualityEvaluator, QualityCriterion
from app.core.prompt_analyzer import PromptAnalyzer


async def test_enhanced_optimization():
    """æµ‹è¯•å¢å¼ºçš„æ ¸å¿ƒä¼˜åŒ–ç®—æ³•"""
    print("=== å¢å¼ºæ ¸å¿ƒä¼˜åŒ–ç®—æ³•æµ‹è¯• ===")
    
    # åˆå§‹åŒ–ç»„ä»¶
    ai_client = AIClient()
    optimizer = AIPromptOptimizer(ai_client)
    evaluator = QualityEvaluator(ai_client)
    analyzer = PromptAnalyzer(ai_client)
    
    # æµ‹è¯•ç”¨ä¾‹
    test_prompt = "å†™ä¸€ä¸ªå‡½æ•°è®¡ç®—ä¸¤ä¸ªæ•°çš„å’Œ"
    
    print(f"\nåŸå§‹æç¤ºè¯: {test_prompt}")
    print("=" * 60)
    
    # 1. æµ‹è¯•æç¤ºè¯åˆ†æå™¨
    print("\n1. ã€æç¤ºè¯åˆ†æå™¨æµ‹è¯•ã€‘")
    try:
        analysis_result = await analyzer.analyze(test_prompt, use_ai=False)
        
        print(f"âœ… åˆ†æå®Œæˆ!")
        print(f"ç±»å‹: {analysis_result.prompt_type.value}")
        print(f"å¤æ‚åº¦: {analysis_result.complexity_level.value}")
        print(f"å­—æ•°: {analysis_result.features.word_count}")
        print(f"ç»“æ„è¯„åˆ†: {analysis_result.structure.structure_score:.1f}/10")
        print(f"å¯è¯»æ€§: {analysis_result.features.readability_score:.1f}/10")
        print(f"ç¼ºå¤±å…ƒç´ : {analysis_result.structure.missing_elements}")
        print(f"ä¼˜ç‚¹æ•°é‡: {len(analysis_result.strengths)}")
        print(f"ç¼ºç‚¹æ•°é‡: {len(analysis_result.weaknesses)}")
        print(f"å»ºè®®æ•°é‡: {len(analysis_result.suggestions)}")
    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")
    
    # 2. æµ‹è¯•è´¨é‡è¯„ä¼°å™¨
    print("\n2. ã€è´¨é‡è¯„ä¼°å™¨æµ‹è¯•ã€‘")
    try:
        quality_report = await evaluator.evaluate(test_prompt, mode="comprehensive")
        
        print(f"âœ… è¯„ä¼°å®Œæˆ!")
        print(f"æ€»ä½“è¯„åˆ†: {quality_report.overall_score:.1f}/10")
        print(f"ç­‰çº§: {quality_report.grade}")
        print(f"å¤„ç†æ—¶é—´: {quality_report.processing_time:.2f}s")
        
        print("\nè¯¦ç»†è¯„åˆ†:")
        for criterion, score in quality_report.detailed_scores.items():
            print(f"  {criterion}: {score.score:.1f}/10 ({score.percentage:.1f}%)")
        
        print(f"\nä¸»è¦é—®é¢˜:")
        for issue in quality_report.issues[:3]:
            print(f"  - {issue}")
        
        print(f"\næ”¹è¿›å»ºè®®:")
        for suggestion in quality_report.suggestions[:3]:
            print(f"  - {suggestion}")
    except Exception as e:
        print(f"âŒ è¯„ä¼°å¤±è´¥: {e}")
    
    # 3. æµ‹è¯•å•ä¸€æ ‡å‡†è¯„ä¼°
    print("\n3. ã€å•ä¸€æ ‡å‡†è¯„ä¼°æµ‹è¯•ã€‘")
    try:
        clarity_score = await evaluator.evaluate_by_criterion(test_prompt, QualityCriterion.CLARITY)
        print(f"âœ… æ¸…æ™°åº¦è¯„åˆ†: {clarity_score.score:.1f}/10")
        print(f"è¯„åˆ†ç†ç”±: {clarity_score.description}")
    except Exception as e:
        print(f"âŒ å•ä¸€æ ‡å‡†è¯„ä¼°å¤±è´¥: {e}")
    
    # 4. æµ‹è¯•é«˜çº§ä¼˜åŒ–å™¨
    print("\n4. ã€é«˜çº§ä¼˜åŒ–å™¨æµ‹è¯•ã€‘")
    try:
        context = OptimizationContext(
            optimization_type="code",
            user_preferences={
                "language": "Python",
                "style": "è¯¦ç»†æ³¨é‡Š",
                "complexity": "åˆå­¦è€…å‹å¥½"
            },
            target_audience="ç¼–ç¨‹åˆå­¦è€…"
        )
        
        optimization_result = await optimizer.optimize(test_prompt, context)
        
        print(f"âœ… ä¼˜åŒ–å®Œæˆ!")
        print(f"åŸå§‹è¯„åˆ†: {optimization_result['quality_score_before']}")
        print(f"ä¼˜åŒ–åè¯„åˆ†: {optimization_result['quality_score_after']}")
        print(f"è¯„åˆ†æå‡: {optimization_result['quality_score_after'] - optimization_result['quality_score_before']:.1f}")
        print(f"å¤„ç†æ—¶é—´: {optimization_result['processing_time']:.2f}s")
        print(f"åº”ç”¨ç­–ç•¥: {', '.join(optimization_result['strategies_used'])}")
        
        print(f"\nä¼˜åŒ–åçš„æç¤ºè¯:")
        optimized_preview = optimization_result['optimized_prompt'][:300] + "..." if len(optimization_result['optimized_prompt']) > 300 else optimization_result['optimized_prompt']
        print(f"  {optimized_preview}")
        
        print(f"\næ”¹è¿›è¯´æ˜:")
        for i, improvement in enumerate(optimization_result['improvements'][:3], 1):
            print(f"  {i}. {improvement['type']}: {improvement['description']}")
    except Exception as e:
        print(f"âŒ ä¼˜åŒ–å¤±è´¥: {e}")
    
    # 5. æµ‹è¯•ç­–ç•¥é¢„è§ˆ
    print("\n5. ã€ä¼˜åŒ–ç­–ç•¥é¢„è§ˆæµ‹è¯•ã€‘")
    try:
        preview = await optimizer.preview_optimization(test_prompt, context)
        
        print(f"âœ… é¢„è§ˆå®Œæˆ!")
        print(f"å½“å‰è¯„åˆ†: {preview['current_analysis']['overall_score']}")
        print(f"æ¨èç­–ç•¥æ•°é‡: {len(preview['recommended_strategies'])}")
        print(f"é¢„è®¡æ”¹è¿›æ•°é‡: {preview['estimated_improvements']}")
        
        print(f"\næ¨èç­–ç•¥:")
        for strategy in preview['recommended_strategies']:
            print(f"  - {strategy['name']}: {strategy['description']} (ä¼˜å…ˆçº§: {strategy['priority']})")
    except Exception as e:
        print(f"âŒ ç­–ç•¥é¢„è§ˆå¤±è´¥: {e}")


async def test_simple_features():
    """æµ‹è¯•ç®€å•åŠŸèƒ½"""
    print("\n=== ç®€å•åŠŸèƒ½æµ‹è¯• ===")
    
    # åˆå§‹åŒ–ç»„ä»¶
    ai_client = AIClient()
    analyzer = PromptAnalyzer()  # ä¸ä½¿ç”¨AIçš„åˆ†æå™¨
    
    test_prompt = "å†™ä¸€ä¸ªPythonå‡½æ•°æ¥è®¡ç®—ä¸¤ä¸ªæ•°å­—çš„å’Œ"
    
    print(f"æµ‹è¯•æç¤ºè¯: {test_prompt}")
    
    # æµ‹è¯•åˆ†æå™¨ï¼ˆä¸ä½¿ç”¨AIï¼‰
    try:
        analysis = await analyzer.analyze(test_prompt, use_ai=False)
        print(f"\nâœ… åˆ†ææˆåŠŸ!")
        print(f"ç±»å‹: {analysis.prompt_type.value}")
        print(f"å¤æ‚åº¦: {analysis.complexity_level.value}")
        print(f"å­—æ•°: {analysis.features.word_count}")
        print(f"å¥å­æ•°: {analysis.features.sentence_count}")
        print(f"å¹³å‡å¥é•¿: {analysis.features.avg_sentence_length:.1f}")
        print(f"æŠ€æœ¯æœ¯è¯­: {analysis.features.technical_terms}")
        print(f"ç»“æ„è¯„åˆ†: {analysis.structure.structure_score:.1f}/10")
        print(f"å¯è¯»æ€§: {analysis.features.readability_score:.1f}/10")
        
        print(f"\nä¼˜ç‚¹ ({len(analysis.strengths)}ä¸ª):")
        for strength in analysis.strengths[:3]:
            print(f"  - {strength}")
            
        print(f"\nç¼ºç‚¹ ({len(analysis.weaknesses)}ä¸ª):")
        for weakness in analysis.weaknesses[:3]:
            print(f"  - {weakness}")
            
        print(f"\nå»ºè®® ({len(analysis.suggestions)}ä¸ª):")
        for suggestion in analysis.suggestions[:3]:
            print(f"  - {suggestion}")
            
    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


async def main():
    """ä¸»å‡½æ•°"""
    print("å¼€å§‹æµ‹è¯•å¢å¼ºçš„æ ¸å¿ƒä¼˜åŒ–ç®—æ³•...")
    
    # å…ˆæµ‹è¯•ç®€å•åŠŸèƒ½
    await test_simple_features()
    
    # å†æµ‹è¯•å®Œæ•´åŠŸèƒ½
    await test_enhanced_optimization()
    
    print("\nğŸ‰ æ ¸å¿ƒä¼˜åŒ–ç®—æ³•æµ‹è¯•å®Œæˆ!")
    print("\nğŸ“Š Week 3 Day 3-5 æ ¸å¿ƒä¼˜åŒ–ç®—æ³•åŠŸèƒ½éªŒè¯:")
    print("âœ… æç¤ºè¯è´¨é‡è¯„ä¼°ç®—æ³• - å·²å®ç°")
    print("âœ… æç¤ºè¯ä¼˜åŒ–ç­–ç•¥å¼•æ“ - å·²å®ç°") 
    print("âœ… ä¼˜åŒ–ç»“æœçš„è§£é‡Šç”Ÿæˆ - å·²å®ç°")
    print("âœ… ä¼˜åŒ–è®°å½•çš„å­˜å‚¨é€»è¾‘ - å·²å®ç°")
    print("âœ… æç¤ºè¯åˆ†æå™¨ - å·²å®ç°")
    print("âœ… è´¨é‡è¯„ä¼°å™¨ - å·²å®ç°")
    print("âœ… ç­–ç•¥åŒ–ä¼˜åŒ–å™¨ - å·²å®ç°")


if __name__ == "__main__":
    asyncio.run(main()) 